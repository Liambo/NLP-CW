{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZxak6MGHQTb",
    "outputId": "574cd88b-e650-4af1-8292-8108b94d60d3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import scipy\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'  # Required to make some modules work together with MacOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "984GfiBgIE02",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_body = pd.read_csv('tr_b.csv') # Loads datasets stored locally (these are just the datasets from the fake news challenge website)\n",
    "train_stances = pd.read_csv('tr_s.csv')\n",
    "test_body = pd.read_csv('te_b.csv')\n",
    "test_stances = pd.read_csv('te_s.csv')\n",
    "\n",
    "punctuation = ['\"', \"'\", '.', '-', '!', '?', '#', '/', ':', ';', '(', ')', '*', '&', '@', '_', ',', '’', '”', '“', '—', '–', '[', ']']\n",
    "stop_words = list(stopwords.words('english')) # List of punctuation & stop words to be removed\n",
    "\n",
    "for i in range(len(train_body)):  # Removing punctuation & stop words from headlines & article bodies\n",
    "    train_body['articleBody'][i] = ''.join(filter(lambda a: a not in punctuation, train_body['articleBody'][i]))\n",
    "    train_body['articleBody'][i] = ' '.join(filter(lambda a: a.lower() not in stop_words, train_body['articleBody'][i].split()))\n",
    "for i in range(len(test_body)):\n",
    "    test_body['articleBody'][i] = ''.join(filter(lambda a: a not in punctuation, test_body['articleBody'][i]))\n",
    "    test_body['articleBody'][i] = ' '.join(filter(lambda a: a.lower() not in stop_words, test_body['articleBody'][i].split()))\n",
    "for i in range(len(train_body)):\n",
    "    train_stances['Headline'][i] = ''.join(filter(lambda a: a not in punctuation, train_stances['Headline'][i]))\n",
    "    train_stances['Headline'][i] = ' '.join(filter(lambda a: a.lower() not in stop_words, train_stances['Headline'][i].split()))\n",
    "for i in range(len(train_body)):\n",
    "    test_stances['Headline'][i] = ''.join(filter(lambda a: a not in punctuation, test_stances['Headline'][i]))\n",
    "    test_stances['Headline'][i] = ' '.join(filter(lambda a: a.lower() not in stop_words, test_stances['Headline'][i].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6haee3QXM0Y3"
   },
   "outputs": [],
   "source": [
    "train_corpus = list(train_body['articleBody']) + list(train_stances['Headline']) # Train corpus to train TF-IDF vectoriser on. We don't use words from test set for obvious reasons.\n",
    "\n",
    "tfIdfVectorizer = TfidfVectorizer()\n",
    "tfIdfVectorizer.fit(train_corpus)\n",
    "\n",
    "train_stances['Headline_TFIDF'] = train_stances.apply(lambda row: tfIdfVectorizer.transform([row.Headline]), axis=1) # Applying TF-IDF vectoriser to all our text data\n",
    "train_body['Body_TFIDF'] = train_body.apply(lambda row: tfIdfVectorizer.transform([row.articleBody]), axis=1) # This returns our vectors in sparse matrix form, so they take up way less space\n",
    "test_stances['Headline_TFIDF'] = test_stances.apply(lambda row: tfIdfVectorizer.transform([row.Headline]), axis=1)\n",
    "test_body['Body_TFIDF'] = test_body.apply(lambda row: tfIdfVectorizer.transform([row.articleBody]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "2ac0e4eec07b455397b7bee4f1ee3241",
      "a616f075d3624c9792af821edb20f7ce",
      "33a4f5600c7241069510794b2fe42d2d",
      "6ac4d513222c4e338fc1ae9605d1e927",
      "a19c5eec9ac9481ab2123868451c805c",
      "f236588e5ff848f5a1d00a460b45bb3c",
      "3c944741704b4f9e8f6048c06d5da60c",
      "c0638d2e4d4446d8a77a53b638b56a9f",
      "f1b5ad05a33246409de0885e47abcc3a",
      "11e01317c9eb4756b6f46691e328d7fe",
      "99378c54802f418ebb5c025c66dcd54b",
      "d49083c561ae4265ada6d44c9aad0dc8",
      "531ba8322c6846d48302bc956432b4d9",
      "50ab21ce51b94106849f7ff2448e2557",
      "a1f01735a04e40fba8c032c4e95a2bc2",
      "d04daee59e1c4f379f892b8fc0327d55",
      "3a40b623c92a4a14aafcf2f407c3d7d3",
      "e2619ccf408c4e2fb92453fd426a3905",
      "8a31363c36ba4d60bb420117c4a4acc8",
      "7d7e6ad78f2d4d18b99b4663b5040298",
      "73e2be78bce74db4bb71ba43010cff91",
      "545d6ca9317441e18fa69bc187bf5327",
      "ec98f9dd4eab426f954df0c2396e2330",
      "4540370520ad4192b81bbb19a6dbee1a",
      "ea1a2f1d75ef4ae1bc2225ad0bf71e2b",
      "1ae530155b5e44b6bb16f697da5d0777",
      "0e558dc0b9864fc9b911d6a2a7116f1d",
      "a5ccccb065ea4cadb4457e4aa4f7699e",
      "f586602a26a0406f9658dca4d30450d2",
      "9a352506f4a447189c16f7f80ad7c0b1",
      "d9dda2d839aa476894d95a5afae72e83",
      "245bd8f284654638b31ad9750d3ca1b5",
      "cab8841ca88f4ee89118e89023a571c1"
     ]
    },
    "id": "XCeXChWEk41Q",
    "outputId": "53f799be-a171-429c-dc36-c81def6966dd"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./bert-base-uncased') # Installing bert tokenizer stored locally. Identical to model found at https://huggingface.co/bert-base-uncased\n",
    "\n",
    "train_stances['Headline_Tokenized'] = train_stances.apply(lambda row: tokenizer.tokenize('[CLS] ' + row.Headline + ' [SEP]'), axis=1) # Tokenizing headlines. We don't limit to 512 tokens here, as no headlines are anywhere enar that long.\n",
    "train_stances['Headline_Indexed'] = train_stances.apply(lambda row: tokenizer.convert_tokens_to_ids(row.Headline_Tokenized), axis=1) # Converting tokens to IDs.\n",
    "train_body['Body_Tokenized'] = train_body.apply(lambda row: tokenizer.tokenize('[CLS] ' + row.articleBody)[:511] + ['[SEP]'], axis=1) # When tokenizing bodies, we tokenize full text before cutting off first 511 tokens and adding final [SEP] token.\n",
    "train_body['Body_Indexed'] = train_body.apply(lambda row: tokenizer.convert_tokens_to_ids(row.Body_Tokenized), axis=1)\n",
    "test_stances['Headline_Tokenized'] = test_stances.apply(lambda row: tokenizer.tokenize('[CLS] ' + row.Headline + ' [SEP]'), axis=1)\n",
    "test_stances['Headline_Indexed'] = test_stances.apply(lambda row: tokenizer.convert_tokens_to_ids(row.Headline_Tokenized), axis=1)\n",
    "test_body['Body_Tokenized'] = test_body.apply(lambda row: tokenizer.tokenize('[CLS] ' + row.articleBody)[:511] + ['[SEP]'], axis=1)\n",
    "test_body['Body_Indexed'] = test_body.apply(lambda row: tokenizer.convert_tokens_to_ids(row.Body_Tokenized), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_body.to_pickle(os.getcwd() + '/train_body.pkl') # Pickling all our tokens so we don't have to do preprocessing steps every time while testing.\n",
    "test_body.to_pickle(os.getcwd() + '/test_body.pkl') # Running this will produce 4 files totalling around 50MB in size.\n",
    "train_stances.to_pickle(os.getcwd() + '/train_stances.pkl')\n",
    "test_stances.to_pickle(os.getcwd() + '/test_stances.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_body = pd.read_pickle('train_body.pkl') # Load all tokens from pickles. For resuming testing from this point without bothering with lengthy preprocessing.\n",
    "train_stances = pd.read_pickle('train_stances.pkl')\n",
    "test_body = pd.read_pickle('test_body.pkl')\n",
    "test_stances = pd.read_pickle('test_stances.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = BertModel.from_pretrained('./bert-base-uncased',\n",
    "                                  output_hidden_states = True,\n",
    "                                  ) # Again using BERT model from https://huggingface.co/bert-base-uncased for embeddings.\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def get_BERT_embedding(model, indexed_tokens):\n",
    "    segments_ids = [1] * len(indexed_tokens) # Segment IDs denote which sentence each token is from, as this model can take 2 sentences as input. we treat entire input as 1 sentence for ease, however.\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    return outputs[0] # First element of output is final layer from BERT model for each timestep. We will use this as our embedding. Some research shows using concatenation of last 4 layers can give better performance, but would rather keep final model simpler & faster to train and spend more time tuning hyperparameters.\n",
    "\n",
    "# WARNING: Following piece of code saves all BERT embeddings as tensor files. This creates around 7GB of files and needs folders named train_stances, test_stances, test_body and train_body.\n",
    "\n",
    "with torch.no_grad():\n",
    "    i=0\n",
    "    path = os.getcwd() + '/train_stances/'\n",
    "    for i in range(len(train_stances)):\n",
    "        get_BERT_embedding(model, train_stances['Headline_Indexed'][i], device, path + str(i) + '.pt') # We save each headline and body as a seperate tensor. This way we can avoid keeping entire dataset in memory when training model.\n",
    "    i=0\n",
    "    path = os.getcwd() + '/test_stances/'\n",
    "    for i in range(len(test_stances)):\n",
    "        get_BERT_embedding(model, test_stances['Headline_Indexed'][i], device, path + str(i) + '.pt')\n",
    "    i=0\n",
    "    path = os.getcwd() + '/test_body/'\n",
    "    for i in range(len(test_body)):\n",
    "        get_BERT_embedding(model, test_body['Body_Indexed'][i], device, path + str(i) + '.pt')\n",
    "    i=0\n",
    "    path = os.getcwd() + '/train_body/'\n",
    "    for i in range(len(train_body)):\n",
    "        get_BERT_embedding(model, train_body['Body_Indexed'][i], device, path + str(i) + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_set = pd.DataFrame(columns=['Vector', 'Label']) # Creating dataframe to hold TF-IDF dataset in.\n",
    "tfidf_test_set = pd.DataFrame(columns=['Vector', 'Label'])\n",
    "label_dict = {'unrelated': 0, 'agree': 1, 'disagree': 1, 'discuss': 1}\n",
    "\n",
    "\n",
    "# We will create the train & test sets by concatenating the vectors of the headline and body. We will store these as scipy sparse matrices to save on space.\n",
    "tfidf_train_set['Vector'] = train_stances.apply(lambda row: scipy.sparse.csr_matrix(np.concatenate((row['Headline_TFIDF'].todense(), train_body['Body_TFIDF'][train_body.index[train_body['Body ID'] == row['Body ID']][0]].todense()), axis=1).A1), axis=1)\n",
    "tfidf_test_set['Vector'] = test_stances.apply(lambda row: scipy.sparse.csr_matrix(np.concatenate((row['Headline_TFIDF'].todense(), test_body['Body_TFIDF'][test_body.index[test_body['Body ID'] == row['Body ID']][0]].todense()), axis=1).A1), axis=1)\n",
    "\n",
    "# Storing labels of the headlines on each row.\n",
    "tfidf_train_set['Label'] = train_stances.apply(lambda row: label_dict[row['Stance']], axis=1)\n",
    "tfidf_test_set['Label'] = test_stances.apply(lambda row: label_dict[row['Stance']], axis=1)\n",
    "\n",
    "# Converting to lists for use with logistic regression model.\n",
    "tfidf_train_inputs = tfidf_train_set['Vector'].to_list()\n",
    "train_labels = tfidf_train_set['Label'].to_list()\n",
    "tfidf_test_inputs = tfidf_test_set['Vector'].to_list()\n",
    "test_labels = tfidf_test_set['Label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_body_id_list = train_stances['Body ID'].to_list() # This creates a list of the Body IDs associated with eachheadline, in order.\n",
    "test_body_id_list = test_stances['Body ID'].to_list()\n",
    "\n",
    "for i in range(len(train_body_id_list)): # We then use said list to find the index of the relevant bodies. rather than finding them by their IDs each time, we can just access the relevant row in the body dataframe which is faster.\n",
    "    train_body_id_list[i] = train_body.index[train_body['Body ID'] == train_body_id_list[i]][0]\n",
    "for i in range(len(test_body_id_list)):\n",
    "    test_body_id_list[i] = test_body.index[test_body['Body ID'] == test_body_id_list[i]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_inputs = []\n",
    "bert_test_inputs = []\n",
    "for i in range(len(os.listdir(os.getcwd() + '/train_stances'))):\n",
    "    try: # We will concatenate the BERT headline & body embeddings fro logistic regression model.\n",
    "        head_file = os.getcwd() + '/train_stances/' + str(i) + '.pt'\n",
    "        body_file = os.getcwd() + '/train_body/' + str(train_body_id_list[i]) + '.pt' # Our body index list is useful here as the body files are saved by index rather than body ID.\n",
    "        head = torch.load(head_file)[0][0] # We take only the embedding of the [CLS] token at the start of the input sequence. This token tends to store context information for the entire input.\n",
    "        body = torch.load(body_file)[0][0] # If we used the entire concatenated input, our embeddings would all be of different lengths which is incompatible with the logistic regression model.\n",
    "        bert_train_inputs.append(np.array(torch.cat((head, body))))\n",
    "    except:\n",
    "        continue\n",
    "for i in range(len(os.listdir(os.getcwd() + '/test_stances'))):\n",
    "    try:\n",
    "        head_file = os.getcwd() + '/test_stances/' + str(i) + '.pt'\n",
    "        body_file = os.getcwd() + '/test_body/' + str(test_body_id_list[i]) + '.pt'\n",
    "        head = torch.load(head_file)[0][0]\n",
    "        body = torch.load(body_file)[0][0]\n",
    "        bert_test_inputs.append(np.array(torch.cat((head, body))))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfidf_train_inputs, open('./train_stuff/tfidf_train_inputs.pkl', 'wb')) # Saving inputs again to avoid preprocessing steps. Creates about 600MB of files. \n",
    "pickle.dump(tfidf_test_inputs, =open('./train_stuff/tfidf_test_inputs.pkl', 'wb'))\n",
    "pickle.dump(train_labels, open('./train_stuff/train_labels.pkl', 'wb'))\n",
    "pickle.dump(test_labels, open('./train_stuff/test_labels.pkl', 'wb'))\n",
    "pickle.dump(train_body_id_list, open('./train_stuff/train_body_id_list.pkl', 'wb'))\n",
    "pickle.dump(test_body_id_list, open('./train_stuff/test_body_id_list.pkl', 'wb'))\n",
    "pickle.dump(bert_train_inputs, open('./train_stuff/bert_train_inputs.pkl', 'wb'))\n",
    "pickle.dump(bert_test_inputs, open('./train_stuff/bert_test_inputs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_inputs = pickle.load(open('./train_stuff/tfidf_train_inputs.pkl', 'rb'))\n",
    "tfidf_test_inputs = pickle.load(open('./train_stuff/tfidf_test_inputs.pkl', 'rb'))\n",
    "train_labels = pickle.load(open('./train_stuff/train_labels.pkl', 'rb'))\n",
    "test_labels = pickle.load(open('./train_stuff/test_labels.pkl', 'rb'))\n",
    "train_body_id_list = pickle.load(open('./train_stuff/train_body_id_list.pkl','rb'))\n",
    "test_body_id_list = pickle.load(open('./train_stuff/test_body_id_list.pkl', 'rb'))\n",
    "bert_train_inputs = pickle.load(open('./train_stuff/bert_train_inputs.pkl', 'rb'))\n",
    "bert_test_inputs = pickle.load(open('./train_stuff/bert_test_inputs.pkl', 'rb'))\n",
    "for i in range(len(tfidf_train_inputs)): # TF-IDF inputs need to be unpacked from sparse matrices to numpy arrays for logistic regression model. This requires >=8GB of memory.\n",
    "    tfidf_train_inputs[i] = np.squeeze(np.asarray(tfidf_train_inputs[i].todense()))\n",
    "for i in range(len(tfidf_test_inputs)):\n",
    "    tfidf_test_inputs[i] = np.squeeze(np.asarray(tfidf_test_inputs[i].todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using logistic regression model from sklearn. We tried scaling the inputs to a normal distribution first, but it didn't do much to improve performance and increased training time.\n",
    "tfidf_classifier = LogisticRegression(solver='liblinear')\n",
    "tfidf_classifier.fit(tfidf_train_inputs, train_labels)\n",
    "tfidf_preds = tfidf_classifier.predict_proba(tfidf_test_inputs) # We return the probabilities rather than classes to run BCEloss later.\n",
    "\n",
    "bert_classifier = LogisticRegression(solver='liblinear', max_iter=10000) # max_iter has to be higher here else we reach recursion limit.\n",
    "bert_classifier.fit(bert_train_inputs, train_labels)\n",
    "bert_preds = bert_classifier.predict_proba(bert_test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Accuracy: 0.67953409672215\n",
      "BERT Accuracy: 0.6159445952858773\n",
      "TF-IDF Precision: 0.3193979933110368\n",
      "BERT Precision: 0.29674306393244876\n",
      "TF-IDF Recall: 0.1351925254813137\n",
      "BERT Recall: 0.2785956964892412\n",
      "TF-IDF BCE Loss: 0.6420563919172942\n",
      "BERT BCE Loss: 0.8968060107975179\n"
     ]
    }
   ],
   "source": [
    "tfidf_TP = 0 # To evaluate models, we will compare accuracy, precision, recall and Binary Cross-Entropy loss.\n",
    "tfidf_FP = 0\n",
    "tfidf_TN = 0\n",
    "tfidf_FN = 0\n",
    "bert_TP = 0\n",
    "bert_FP = 0\n",
    "bert_TN = 0\n",
    "bert_FN = 0\n",
    "for i in range(len(tfidf_preds)):\n",
    "    if test_labels[i] == 1:\n",
    "        if round(tfidf_preds[i][1]) == 1:\n",
    "            tfidf_TP += 1\n",
    "        else:\n",
    "            tfidf_FN += 1\n",
    "        if round(bert_preds[i][1]) == 1:\n",
    "            bert_TP += 1\n",
    "        else:\n",
    "            bert_FN += 1\n",
    "    else:\n",
    "        if round(tfidf_preds[i][0]) == 1:\n",
    "            tfidf_TN += 1\n",
    "        else:\n",
    "            tfidf_FP += 1\n",
    "        if round(bert_preds[i][0]) == 1:\n",
    "            bert_TN += 1\n",
    "        else:\n",
    "            bert_FP += 1\n",
    "print('TF-IDF Accuracy: ' + str((tfidf_TP + tfidf_TN)/len(tfidf_preds)))\n",
    "print('BERT Accuracy: ' + str((bert_TP + bert_TN)/len(tfidf_preds)))\n",
    "print('TF-IDF Precision: ' + str(tfidf_TP/(tfidf_TP + tfidf_FP)))\n",
    "print('BERT Precision: ' + str(bert_TP/(bert_TP + bert_FP)))\n",
    "print('TF-IDF Recall: ' + str(tfidf_TP/(tfidf_TP + tfidf_FN)))\n",
    "print('BERT Recall: ' + str(bert_TP/(bert_TP + bert_FN)))\n",
    "tfidf_bce_loss = nn.BCELoss(reduction='mean')(torch.tensor(tfidf_preds)[:, 1], torch.tensor(test_labels, dtype=torch.double)).item()\n",
    "bert_bce_loss = nn.BCELoss(reduction='mean')(torch.tensor(bert_preds)[:, 1], torch.tensor(test_labels, dtype=torch.double)).item()\n",
    "print('TF-IDF BCE Loss: ' + str(tfidf_bce_loss))\n",
    "print('BERT BCE Loss: ' + str(bert_bce_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, bert, relevance, hidden_size=256, num_layers=2, dropout=0.1, tfidf_length=0):\n",
    "        super(GRU, self).__init__()\n",
    "        self.bert = bert # indicates whether we are using BERT or  TFIDF embeddings.\n",
    "        self.relevance = relevance # indicates whether we are classifying related/unrelated (True) or agree/disagree/discuss (False).\n",
    "        \n",
    "        if bert: # Model for BERT embeddings trains two seperated 2-layered GRUs, one for headlines and one for bodies.\n",
    "            self.headline_gru = nn.GRU(input_size=768, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "            self.body_gru = nn.GRU(input_size=768, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "            if relevance: # If we are tesing relevance, use a sigmoid function and output one probability of how likely headline is to be relevant.\n",
    "                self.fc = nn.Sequential(nn.Linear(2*hidden_size, 2*hidden_size), # We concatenate final output from both GRUs, so need input size of 2*hidden_size.\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(2*hidden_size, 2*hidden_size),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(2*hidden_size, 1),\n",
    "                                        nn.Sigmoid())\n",
    "            else: # If we are tesing agree/disagree/discuss, we use a softmax function to output a probability distribution over all 3 options.\n",
    "                self.fc = nn.Sequential(nn.Linear(2*hidden_size, 2*hidden_size),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(2*hidden_size, 2*hidden_size),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(2*hidden_size, 3),\n",
    "                                        nn.Softmax())\n",
    "        else: # Only 1 GRU  for TF-IDF embeddings since we input headline then body in sequence.\n",
    "            self.gru = nn.GRU(input_size=tfidf_length, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "            if relevance:\n",
    "                self.fc = nn.Sequential(nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(hidden_size, 1),\n",
    "                                        nn.Sigmoid())\n",
    "            else:\n",
    "                self.fc = nn.Sequential(nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(hidden_size, 3),\n",
    "                                        nn.Softmax(dim=0))\n",
    "\n",
    "    def forward(self, head, body):\n",
    "        if self.bert:\n",
    "            _, head = self.headline_gru(head) # We only care about final output, ignore tensor with first t-1 hidden states.\n",
    "            _, body = self.body_gru(body)\n",
    "            pred = self.fc(torch.cat((head[-1], body[-1]), dim=0)) # We concatenate final hidden states from the final GRU layer, hence head[-1] & body[-1]\n",
    "        else:\n",
    "            _, x = self.gru(head)\n",
    "            pred = self.fc(x[-1])\n",
    "        return pred\n",
    "\n",
    "class Optimisation: # We use an optimisation class to handle aspects of training as it keeps things neater and more generalisable.\n",
    "    def __init__(self, model, optimiser):\n",
    "        self.model = model\n",
    "        self.optimiser = optimiser\n",
    "        self.bce = nn.BCELoss(reduction='sum') # Our loss function is just BCE loss as it is very effective for binary classification problems such as this.\n",
    "    \n",
    "    def loss_fn(self, x_pred, x_target):\n",
    "        loss = self.bce(x_pred, x_target)\n",
    "        return loss\n",
    "    \n",
    "    def train_step(self, head, body, target): # Training involves optimiser step\n",
    "        self.optimiser.zero_grad()\n",
    "        self.model.train()\n",
    "        pred = self.model(head, body)\n",
    "        loss = self.loss_fn(pred, target)\n",
    "        loss.backward()\n",
    "        self.optimiser.step()\n",
    "        return loss.item(), pred\n",
    "    \n",
    "    def test_step(self, head, body, target): # Testing does not!\n",
    "        self.model.eval()\n",
    "        pred = self.model(head, body)\n",
    "        loss = self.loss_fn(pred, target)\n",
    "        return loss.item(), pred\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset): # Our dataset class is used to handle BERT embeddings. TF-IDF embeddings are just stored in a list, as they take up little memory in scipy.sparse format. \n",
    "    def __init__(self, train, stance_ids, labels, body_ids):\n",
    "        if train: # Separate paths for training & testing mode.\n",
    "            self.stance_path = os.getcwd() + '/train_stances/'\n",
    "            self.body_path = os.getcwd() + '/train_body/'\n",
    "        else:\n",
    "            self.stance_path = os.getcwd() + '/test_stances/'\n",
    "            self.body_path = os.getcwd() + '/test_body/'\n",
    "        self.body_ids = body_ids\n",
    "        self.stance_ids = stance_ids\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index): # Return tuple of headline embedding, body embedding and label.\n",
    "        s = torch.load(self.stance_path + str(self.stance_ids[index]) + '.pt')\n",
    "        b = torch.load(self.body_path + str(self.body_ids[index]) + '.pt')\n",
    "        l = self.labels[index]\n",
    "        return s, b, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GRU(True, True, hidden_size=256, num_layers=2, dropout=0.02).to(device) # BERT model for relevance classifying.\n",
    "optimiser = Optimisation(model, optim.Adam(model.parameters(), lr=0.00001)) # learning rate of 0.00001 and ADAM optimiser worked best after some brief testing.\n",
    "\n",
    "# Train & test dataloaders.\n",
    "train_dataset = Dataset(True, list(range(len(train_labels))), train_labels, train_body_id_list)\n",
    "test_dataset = Dataset(False, list(range(len(test_labels))), test_labels, test_body_id_list)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    loss = 0\n",
    "    i = 0\n",
    "    for batch in train_dataloader: # Move data to device & feed into optimiser for train step.\n",
    "        temp, _ = optimiser.train_step(batch[0].squeeze().to(device), batch[1].squeeze().to(device), batch[2].type(torch.float32).to(device))\n",
    "        loss += temp\n",
    "        i += 1\n",
    "        if i % 100 == 0: # Print loss every 100 iterations.\n",
    "            print('avg loss for iteration ' + str(i + epoch * len(train_dataloader)) + ': ' + str(loss/100))\n",
    "            loss = 0\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for batch in test_dataloader: # Evaluate after every epoch\n",
    "        temp, pred = optimiser.test_step(batch[0].squeeze().to(device), batch[1].squeeze().to(device), batch[2].type(torch.float32).to(device))\n",
    "        test_loss += temp\n",
    "        if int(batch[2].squeeze()) == round(pred.item()): # Compare label to predicted label.\n",
    "            correct += 1\n",
    "    print('BERT avg loss for epoch ' + str(epoch) + ': ' + str(test_loss/len(test_dataloader)))\n",
    "    print('BERT correct prediction proportion for epoch ' + str(epoch) + ': ' + str(correct/len(test_dataloader)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tfidf_length = tfidf_train_inputs[0].shape[0]//2 # Finds tf-idf vector length for model parameters.\n",
    "model = GRU(False, True, hidden_size=256, num_layers=2, dropout=0.02, tfidf_length=tfidf_length).to(device)\n",
    "optimiser = Optimisation(model, optim.Adam(model.parameters(), lr=0.00001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1): # Works very similar to BERT training loop, except inputs are just taken from tfidf inputs.\n",
    "    loss = 0\n",
    "    for i in range(len(tfidf_train_inputs)): # Reshape inputs as they are still in concatenated head-body form from logistic regression inputs.\n",
    "        temp, pred = optimiser.train_step(torch.reshape(torch.tensor(tfidf_train_inputs[i], dtype=torch.float32, device=device), (2, tfidf_length)), 0, torch.tensor([train_labels[i]], dtype=torch.float32, device=device))\n",
    "        loss += temp\n",
    "        if i % 100 == 0:\n",
    "            print('avg loss for iteration ' + str(i + epoch * len(tfidf_train_inputs)) + ': ' + str(loss/100))\n",
    "            loss = 0\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for i in range(len(tfidf_test_inputs)):\n",
    "        temp, pred = optimiser.test_step(torch.reshape(torch.tensor(tfidf_test_inputs[i], dtype=torch.float32, device=device), (2, tfidf_length)), 0, torch.tensor([test_labels[i]], dtype=torch.float32, device=device))\n",
    "        test_loss += temp\n",
    "        if int(test_labels[i]) == round(pred.item()):\n",
    "            correct += 1\n",
    "    print('TFIDF avg loss for epoch ' + str(epoch) + ': ' + str(test_loss/len(tfidf_test_inputs)))\n",
    "    print('TFIDF correct prediction proportion for epoch ' + str(epoch) + ': ' + str(correct/len(tfidf_test_inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'agree': 0, 'disagree': 1, 'discuss': 2} # Construction of datasets for aggree/disagree/discuss classification\n",
    "classifier_train_stance_ids = []\n",
    "classifier_train_labels = []\n",
    "classifier_train_body_id_list = []\n",
    "classifier_tfidf_train_inputs = []\n",
    "classifier_test_stance_ids = []\n",
    "classifier_test_labels = []\n",
    "classifier_test_body_id_list = []\n",
    "classifier_tfidf_test_inputs = []\n",
    "for i in range(len(train_stances)): # Loop through all headlines looking for only relevant ones\n",
    "    if train_stances['Stance'][i] in ['agree', 'disagree', 'discuss']:\n",
    "        classifier_train_stance_ids.append(i) # Save the 'id' (this is just the index as this is how the pytorch files were named)\n",
    "        classifier_train_labels.append([0, 0, 0]) # Prepared list for label\n",
    "        classifier_train_labels[-1][label_dict[train_stances['Stance'][i]]] = 1 # Put 1 in correct list index\n",
    "        classifier_train_body_id_list.append(train_body.index[train_body['Body ID'] == train_stances['Body ID'][i]][0]) # save 'id' of body (again index in dataframe)\n",
    "        classifier_tfidf_train_inputs.append(tfidf_train_inputs[i]) # Append the tf-idf input, too\n",
    "for i in range(len(test_stances)):\n",
    "    if test_stances['Stance'][i] in ['agree', 'disagree', 'discuss']:\n",
    "        classifier_test_stance_ids.append(i)\n",
    "        classifier_test_labels.append([0, 0, 0])\n",
    "        classifier_test_labels[-1][label_dict[test_stances['Stance'][i]]] = 1\n",
    "        classifier_test_body_id_list.append(test_body.index[test_body['Body ID'] == test_stances['Body ID'][i]][0])\n",
    "        classifier_tfidf_test_inputs.append(tfidf_test_inputs[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier_train_stance_ids, open('./train_stuff/classifier_train_stance_ids.pkl', 'wb')) # Once again, saving them to save on preprocessing time.\n",
    "pickle.dump(classifier_train_labels, open('./train_stuff/classifier_train_labels.pkl', 'wb')) # Creates ~50MB of files.\n",
    "pickle.dump(classifier_train_body_id_list, open('./train_stuff/classifier_train_body_id_list.pkl', 'wb'))\n",
    "pickle.dump(classifier_test_stance_ids, open('./train_stuff/classifier_test_stance_ids.pkl', 'wb'))\n",
    "pickle.dump(classifier_test_labels, open('./train_stuff/classifier_test_labels.pkl', 'wb'))\n",
    "pickle.dump(classifier_test_body_id_list, open('./train_stuff/classifier_test_body_id_list.pkl', 'wb'))\n",
    "pickle.dump(classifier_tfidf_test_inputs, open('./train_stuff/classifier_tfidf_test_inputs.pkl', 'wb'))\n",
    "pickle.dump(classifier_tfidf_train_inputs, open('./train_stuff/classifier_tfidf_train_inputs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_train_stance_ids = pickle.load(open('./train_stuff/classifier_train_stance_ids.pkl', 'rb'))\n",
    "classifier_train_labels = pickle.load(open('./train_stuff/classifier_train_labels.pkl', 'rb'))\n",
    "classifier_train_body_id_list = pickle.load(open('./train_stuff/classifier_train_body_id_list.pkl', 'rb'))\n",
    "classifier_test_stance_ids = pickle.load(open('./train_stuff/classifier_test_stance_ids.pkl', 'rb'))\n",
    "classifier_test_labels = pickle.load(open('./train_stuff/classifier_test_labels.pkl', 'rb'))\n",
    "classifier_test_body_id_list = pickle.load(open('./train_stuff/classifier_test_body_id_list.pkl', 'rb'))\n",
    "classifier_tfidf_test_inputs = pickle.load(open('./train_stuff/classifier_tfidf_test_inputs.pkl', 'rb'))\n",
    "classifier_tfidf_train_inputs = pickle.load(open('./train_stuff/classifier_tfidf_train_inputs.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GRU(True, False).to(device) # GRU for agree/disagree/discuss with BERT inputs\n",
    "optimiser = Optimisation(model, optim.Adam(model.parameters(), lr=0.00001))\n",
    "\n",
    "train_dataset = Dataset(True, classifier_train_stance_ids, classifier_train_labels, classifier_train_body_id_list)\n",
    "test_dataset = Dataset(False, classifier_test_stance_ids, classifier_test_labels, classifier_test_body_id_list)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3): # Mainly same as before.\n",
    "    loss = 0\n",
    "    i = 0\n",
    "    for batch in train_dataloader:\n",
    "        temp, pred = optimiser.train_step(batch[0].squeeze().to(device), batch[1].squeeze().to(device), torch.tensor(batch[2], device=device, dtype=torch.float32))\n",
    "        loss += temp\n",
    "        i += 1\n",
    "        if i % 100 == 0:\n",
    "            print('avg loss for iteration ' + str(i + epoch * len(train_dataloader)) + ': ' + str(loss/100))\n",
    "            loss = 0\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for batch in test_dataloader:\n",
    "        temp, pred = optimiser.test_step(batch[0].squeeze().to(device), batch[1].squeeze().to(device), torch.tensor(batch[2], device=device, dtype=torch.float32))\n",
    "        test_loss += temp # To check if correct, we find indexes of highest values in label and prediction vectors. These will be equal if prediction is correct.\n",
    "        if batch[2].index(max(batch[2])) == pred.tolist().index(max(pred.tolist())):\n",
    "            correct += 1\n",
    "    print('BERT avg loss for epoch ' + str(epoch) + ': ' + str(test_loss/len(test_dataloader)))\n",
    "    print('BERT correct prediction proportion for epoch ' + str(epoch) + ': ' + str(correct/len(test_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "classifier_tfidf_length = classifier_tfidf_train_inputs[0].shape[0]//2\n",
    "model = GRU(False, False, tfidf_length=classifier_tfidf_length).to(device) # GRU for agree/disagree/discuss classification on TF-IDF inputs.\n",
    "optimiser = Optimisation(model, optim.Adam(model.parameters(), lr=0.00001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    loss = 0\n",
    "    for i in range(len(classifier_tfidf_train_inputs)):\n",
    "        temp, pred = optimiser.train_step(torch.reshape(torch.tensor(classifier_tfidf_train_inputs[i], dtype=torch.float32, device=device), (2, classifier_tfidf_length)), 0, torch.tensor(classifier_train_labels[i], dtype=torch.float32, device=device))\n",
    "        loss += temp\n",
    "        if i % 100 == 0:\n",
    "            print('avg loss for iteration ' + str(i + epoch * len(classifier_tfidf_train_inputs)) + ': ' + str(loss/100))\n",
    "            loss = 0\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for i in range(len(classifier_tfidf_test_inputs)):\n",
    "        temp, pred = optimiser.test_step(torch.reshape(torch.tensor(classifier_tfidf_test_inputs[i], dtype=torch.float32, device=device), (2, classifier_tfidf_length)), 0, torch.tensor(classifier_test_labels[i], dtype=torch.float32, device=device))\n",
    "        test_loss += temp\n",
    "        if classifier_test_labels[i].index(max(classifier_test_labels[i])) == pred.tolist().index(max(pred.tolist())):\n",
    "            correct += 1\n",
    "    print('TFIDF avg loss for epoch ' + str(epoch) + ': ' + str(test_loss/len(classifier_tfidf_test_inputs)))\n",
    "    print('TFIDF correct prediction proportion for epoch ' + str(epoch) + ': ' + str(correct/len(classifier_tfidf_test_inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7260349441208878\n",
      "Precision: 0.5458673932788374\n",
      "Recall: 0.08507927519818799\n",
      "F1-Score: 0.1472137170851194\n"
     ]
    }
   ],
   "source": [
    "# Measuring accuracy, precision, recall & F1-score of best TF-IDF model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tfidf_length = tfidf_train_inputs[0].shape[0]//2\n",
    "model = GRU(False, True, hidden_size=256, num_layers=2, dropout=0.1, tfidf_length=tfidf_length).to(device)\n",
    "model.load_state_dict(torch.load('./Model_Checkpoints/0.60 0.726 TFIDF.pt', map_location=device)['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "true_pos = 0\n",
    "true_neg = 0\n",
    "false_pos = 0\n",
    "false_neg = 0\n",
    "for i in range(len(tfidf_test_inputs)):\n",
    "    pred = model(torch.reshape(torch.tensor(tfidf_test_inputs[i], dtype=torch.float32, device=device), (2, tfidf_length)), 0)\n",
    "    if test_labels[i] == 1:\n",
    "        if round(pred.item()) == 1:\n",
    "            true_pos += 1\n",
    "        else:\n",
    "            false_neg += 1\n",
    "    else:\n",
    "        if round(pred.item()) == 1:\n",
    "            false_pos += 1\n",
    "        else:\n",
    "            true_neg += 1\n",
    "print('Accuracy: ' + str(((true_pos + true_neg)/i)))\n",
    "precision = true_pos/(true_pos + false_pos)\n",
    "recall = true_pos/(true_pos+false_neg)\n",
    "print('Precision: ' + str(precision))\n",
    "print('Recall: ' + str(recall))\n",
    "print('F1-Score: ' + str(2*precision*recall/(precision+recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Evaluating performance of best models & doing start-to-finish evaluation.\n",
    "\n",
    "model = GRU(True, True, hidden_size=256, num_layers=2, dropout=0.1).to(device)\n",
    "model.load_state_dict(torch.load('./Model_Checkpoints/0.58 0.734 BERT.pt', map_location=device)['model_state_dict'])\n",
    "model.eval()\n",
    "test_dataset = Dataset(False, list(range(len(test_labels))), test_labels, test_body_id_list)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7349388108448431\n",
      "Precision: 0.5243323442136498\n",
      "Recall: 0.5002831257078143\n",
      "F1-Score: 0.5120254998551145\n"
     ]
    }
   ],
   "source": [
    "relevant_ids = [] # Measuring accuracy, precision, recall & f1 score and producing list of predicted relevant IDs for stance classification\n",
    "true_pos = 0\n",
    "false_pos = 0\n",
    "true_neg = 0\n",
    "false_neg = 0\n",
    "correct = 0\n",
    "i = 0\n",
    "true = []\n",
    "predicted = []\n",
    "for batch in test_dataloader:\n",
    "    pred = model(batch[0].squeeze().to(device), batch[1].squeeze().to(device))\n",
    "    true.append(3 if batch[2].item() == 0 else -1)\n",
    "    predicted.append(3 if round(pred.item()) == 0 else -1)\n",
    "    if round(pred.item()) == 1:\n",
    "        relevant_ids.append(i)\n",
    "    if batch[2].item() == 1:\n",
    "        if round(pred.item()) == 1:\n",
    "            true_pos += 1\n",
    "        else:\n",
    "            false_neg += 1\n",
    "    else:\n",
    "        if round(pred.item()) == 1:\n",
    "            false_pos += 1\n",
    "        else:\n",
    "            true_neg += 1\n",
    "            correct += 1 # This is only correct one here, the 'relevant' ones aren't done with classification yet\n",
    "    i += 1\n",
    "print('Accuracy: ' + str(((true_pos + true_neg)/i)))\n",
    "precision = true_pos/(true_pos + false_pos)\n",
    "recall = true_pos/(true_pos+false_neg)\n",
    "print('Precision: ' + str(precision))\n",
    "print('Recall: ' + str(recall))\n",
    "print('F1-Score: ' + str(2*precision*recall/(precision+recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'agree': 0, 'disagree': 1, 'discuss': 2, 'unrelated': 3}\n",
    "relevant_body_ids = []\n",
    "relevant_labels = []\n",
    "for i in relevant_ids:\n",
    "    relevant_labels.append([0, 0, 0, 0])\n",
    "    relevant_labels[-1][label_dict[test_stances['Stance'][i]]] = 1\n",
    "    relevant_body_ids.append(test_body.index[test_body['Body ID'] == test_stances['Body ID'][i]][0])\n",
    "for i in range(len(true)):\n",
    "    if true[i] == -1:\n",
    "        true[i] = label_dict[test_stances['Stance'][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU(True, False, hidden_size=128, num_layers=2, dropout=0.1).to(device)\n",
    "model.load_state_dict(torch.load('./Model_Checkpoints/1.41 0.655 CLASSIFIER_BERT.pt')['model_state_dict'])\n",
    "model.eval()\n",
    "test_dataset = Dataset(False, relevant_ids, relevant_labels, relevant_body_ids)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/cgcr45/NLP/env/lib/python3.8/site-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6937787746428993\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       agree       0.21      0.26      0.23      1903\n",
      "    disagree       0.00      0.00      0.00       697\n",
      "     discuss       0.45      0.45      0.45      4464\n",
      "   unrelated       0.81      0.83      0.82     18349\n",
      "\n",
      "    accuracy                           0.69     25413\n",
      "   macro avg       0.37      0.38      0.37     25413\n",
      "weighted avg       0.68      0.69      0.69     25413\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/cgcr45/NLP/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/cgcr45/NLP/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/cgcr45/NLP/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report # Much easier than doing 4 precision/recall calculations by hand...\n",
    "i=0\n",
    "for batch in test_dataloader:\n",
    "        pred = model(batch[0].squeeze().to(device), batch[1].squeeze().to(device))\n",
    "        true[relevant_ids[i]] = relevant_labels[i].index(max(relevant_labels[i]))\n",
    "        predicted[relevant_ids[i]] = pred.tolist().index(max(pred.tolist()))\n",
    "        if relevant_labels[i].index(max(relevant_labels[i])) == pred.tolist().index(max(pred.tolist())):\n",
    "            correct += 1\n",
    "        i += 1\n",
    "print(correct/len(test_stances))\n",
    "report = classification_report(true, predicted, target_names=['agree', 'disagree', 'discuss', 'unrelated'])\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0e558dc0b9864fc9b911d6a2a7116f1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11e01317c9eb4756b6f46691e328d7fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ae530155b5e44b6bb16f697da5d0777": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_245bd8f284654638b31ad9750d3ca1b5",
      "placeholder": "​",
      "style": "IPY_MODEL_cab8841ca88f4ee89118e89023a571c1",
      "value": " 570/570 [00:00&lt;00:00, 2.07kB/s]"
     }
    },
    "245bd8f284654638b31ad9750d3ca1b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ac0e4eec07b455397b7bee4f1ee3241": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a616f075d3624c9792af821edb20f7ce",
       "IPY_MODEL_33a4f5600c7241069510794b2fe42d2d",
       "IPY_MODEL_6ac4d513222c4e338fc1ae9605d1e927"
      ],
      "layout": "IPY_MODEL_a19c5eec9ac9481ab2123868451c805c"
     }
    },
    "33a4f5600c7241069510794b2fe42d2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0638d2e4d4446d8a77a53b638b56a9f",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f1b5ad05a33246409de0885e47abcc3a",
      "value": 231508
     }
    },
    "3a40b623c92a4a14aafcf2f407c3d7d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c944741704b4f9e8f6048c06d5da60c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4540370520ad4192b81bbb19a6dbee1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5ccccb065ea4cadb4457e4aa4f7699e",
      "placeholder": "​",
      "style": "IPY_MODEL_f586602a26a0406f9658dca4d30450d2",
      "value": "Downloading: 100%"
     }
    },
    "50ab21ce51b94106849f7ff2448e2557": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a31363c36ba4d60bb420117c4a4acc8",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7d7e6ad78f2d4d18b99b4663b5040298",
      "value": 28
     }
    },
    "531ba8322c6846d48302bc956432b4d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a40b623c92a4a14aafcf2f407c3d7d3",
      "placeholder": "​",
      "style": "IPY_MODEL_e2619ccf408c4e2fb92453fd426a3905",
      "value": "Downloading: 100%"
     }
    },
    "545d6ca9317441e18fa69bc187bf5327": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6ac4d513222c4e338fc1ae9605d1e927": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11e01317c9eb4756b6f46691e328d7fe",
      "placeholder": "​",
      "style": "IPY_MODEL_99378c54802f418ebb5c025c66dcd54b",
      "value": " 226k/226k [00:00&lt;00:00, 765kB/s]"
     }
    },
    "73e2be78bce74db4bb71ba43010cff91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d7e6ad78f2d4d18b99b4663b5040298": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8a31363c36ba4d60bb420117c4a4acc8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99378c54802f418ebb5c025c66dcd54b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a352506f4a447189c16f7f80ad7c0b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a19c5eec9ac9481ab2123868451c805c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1f01735a04e40fba8c032c4e95a2bc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73e2be78bce74db4bb71ba43010cff91",
      "placeholder": "​",
      "style": "IPY_MODEL_545d6ca9317441e18fa69bc187bf5327",
      "value": " 28.0/28.0 [00:00&lt;00:00, 214B/s]"
     }
    },
    "a5ccccb065ea4cadb4457e4aa4f7699e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a616f075d3624c9792af821edb20f7ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f236588e5ff848f5a1d00a460b45bb3c",
      "placeholder": "​",
      "style": "IPY_MODEL_3c944741704b4f9e8f6048c06d5da60c",
      "value": "Downloading: 100%"
     }
    },
    "c0638d2e4d4446d8a77a53b638b56a9f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cab8841ca88f4ee89118e89023a571c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d04daee59e1c4f379f892b8fc0327d55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d49083c561ae4265ada6d44c9aad0dc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_531ba8322c6846d48302bc956432b4d9",
       "IPY_MODEL_50ab21ce51b94106849f7ff2448e2557",
       "IPY_MODEL_a1f01735a04e40fba8c032c4e95a2bc2"
      ],
      "layout": "IPY_MODEL_d04daee59e1c4f379f892b8fc0327d55"
     }
    },
    "d9dda2d839aa476894d95a5afae72e83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e2619ccf408c4e2fb92453fd426a3905": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea1a2f1d75ef4ae1bc2225ad0bf71e2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9a352506f4a447189c16f7f80ad7c0b1",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d9dda2d839aa476894d95a5afae72e83",
      "value": 570
     }
    },
    "ec98f9dd4eab426f954df0c2396e2330": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4540370520ad4192b81bbb19a6dbee1a",
       "IPY_MODEL_ea1a2f1d75ef4ae1bc2225ad0bf71e2b",
       "IPY_MODEL_1ae530155b5e44b6bb16f697da5d0777"
      ],
      "layout": "IPY_MODEL_0e558dc0b9864fc9b911d6a2a7116f1d"
     }
    },
    "f1b5ad05a33246409de0885e47abcc3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f236588e5ff848f5a1d00a460b45bb3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f586602a26a0406f9658dca4d30450d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
